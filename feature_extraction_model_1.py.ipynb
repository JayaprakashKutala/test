{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayap\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\jayap\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import spacy, csv \n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round \n",
    "from __future__ import unicode_literals, print_function\n",
    "import nltk\n",
    "import re\n",
    "import io\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the text into sentences, using \n",
    "# Spacy's sentence segmentation which can \n",
    "# be found at https://spacy.io/usage/spacy-101 \n",
    "def break_sentences(text): \n",
    "    global nlp\n",
    "    doc = nlp(text) \n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Number of Words in the text \n",
    "def word_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    words = 0\n",
    "    for sentence in sentences: \n",
    "        words += len([token for token in sentence]) \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of sentences in the text \n",
    "def sentence_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns average sentence length \n",
    "def avg_sentence_length(text): \n",
    "    words = word_count(text) \n",
    "    sentences = sentence_count(text) \n",
    "    average_sentence_length = float(words / sentences) \n",
    "    return average_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textstat is a python package, to calculate statistics from \n",
    "# text to determine readability, \n",
    "# complexity and grade level of a particular corpus. \n",
    "# Package can be found at https://pypi.python.org/pypi/textstat \n",
    "def syllables_count(word): \n",
    "    return textstatistics().syllable_count(str(word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the average number of syllables per \n",
    "# word in the text \n",
    "def avg_syllables_per_word(text): \n",
    "    syllable = syllables_count(text) \n",
    "    words = word_count(text) \n",
    "    ASPW = float(syllable) / float(words) \n",
    "    return legacy_round(ASPW, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return total Difficult Words in a text \n",
    "def difficult_words(text): \n",
    "\n",
    "    # Find all words in the text \n",
    "    words = [] \n",
    "    sentences = break_sentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [token for token in sentence] \n",
    "\n",
    "    # difficult words are those with syllables >= 2 \n",
    "    # easy_word_set is provide by Textstat as \n",
    "    # a list of common words \n",
    "    diff_words_set = set() \n",
    "    \n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word) \n",
    "        if word not in easy_word_set and syllable_count >= 2: \n",
    "            diff_words_set.add(word) \n",
    "\n",
    "    return len(diff_words_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word is polysyllablic if it has more than 3 syllables \n",
    "# this functions returns the number of all such words \n",
    "# present in the text \n",
    "def poly_syllable_count(text): \n",
    "    count = 0\n",
    "    words = [] \n",
    "    sentences = break_sentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [token for token in sentence] \n",
    "\n",
    "\n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3: \n",
    "            count += 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_reading_ease(text): \n",
    "    \"\"\" \n",
    "        Implements Flesch Formula: \n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n",
    "        Here, \n",
    "        ASL = average sentence length (number of words \n",
    "            divided by number of sentences) \n",
    "            ASW = average word length in syllables (number of syllables \n",
    "            divided by number of words) \n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) -float(84.6 * avg_syllables_per_word(text)) \n",
    "    return legacy_round(FRE, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunning_fog(text): \n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words) \n",
    "    return grade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smog_index(text): \n",
    "    \"\"\" \n",
    "        Implements SMOG Formula / Grading \n",
    "        SMOG grading = 3 + ?polysyllable count. \n",
    "        Here, polysyllable count = number of words of more \n",
    "        than two syllables in a sample of 30 sentences. \n",
    "    \"\"\"\n",
    "\n",
    "    if sentence_count(text) >= 3: \n",
    "        poly_syllab = poly_syllable_count(text) \n",
    "        SMOG = (1.043 * (30*(poly_syllab / sentence_count(text)))**0.5) + 3.1291\n",
    "        return legacy_round(SMOG, 1) \n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dale_chall_readability_score(text): \n",
    "    \"\"\" \n",
    "        Implements Dale Challe Formula: \n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365 \n",
    "        Here, PDW = Percentage of difficult words. ASL = Average sentence length \n",
    "    \"\"\"\n",
    "    words = word_count(text) \n",
    "    # Number of words not termed as difficult words \n",
    "    count = words - difficult_words(text) \n",
    "    if words > 0: \n",
    "\n",
    "        # Percentage of words not on difficult word list \n",
    "        per = float(count) / float(words) * 100\n",
    "\n",
    "    # diff_words stores percentage of difficult words \n",
    "    diff_words = 100 - per \n",
    "\n",
    "    raw_score = (0.1579 * diff_words) + (0.0496 * avg_sentence_length(text)) \n",
    "\n",
    "    # If Percentage of Difficult Words is greater than 5 %, then; \n",
    "    # Adjusted Score = Raw Score + 3.6365, \n",
    "    # otherwise Adjusted Score = Raw Score \n",
    "\n",
    "    if diff_words > 5:\n",
    "\n",
    "        raw_score += 3.6365\n",
    "\n",
    "    return legacy_round(raw_score, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming_word(text):\n",
    "    ps = PorterStemmer()\n",
    "    nltk.download('punkt')\n",
    "    nlp = spacy.blank('en')\n",
    "    target = ps.stem('cookies')\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        if ps.stem(w)==target:\n",
    "            return (1)\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pharse_test(nlp,sell_data,not_sell_data,share_data,\n",
    "                not_share_data,purpose,disclosure,security):\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    matcher.add('is_sell', None, *sell_data)\n",
    "    matcher.add('is_not_sell', None, *not_sell_data)\n",
    "    matcher.add('is_share', None, *share_data)\n",
    "    matcher.add('is_not_share', None, *not_share_data)\n",
    "    matcher.add('is_purpose', None, *purpose)\n",
    "    matcher.add('is_disclosure',None,*disclosure)\n",
    "    matcher.add('is_security',None,*security)\n",
    "\n",
    "    list_data = []\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  \n",
    "        list_data.append(rule_id)\n",
    "    \n",
    "\n",
    "    is_sell = int('is_sell' in list_data)\n",
    "    is_not_sell = int('is_not_sell' in list_data)\n",
    "    is_share = int('is_share' in list_data)\n",
    "    is_not_share = int('is_not_share' in list_data)\n",
    "    is_purpose = int('is_purpose' in list_data)\n",
    "    is_disclosure = int('is_disclosure' in list_data)\n",
    "    is_security = int('is_security' in list_data)\n",
    "\n",
    "    return(is_sell,is_not_sell,is_share,is_not_share,is_purpose,is_disclosure,is_security)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['id','is_minor','is_geo-location','is_email','is_vendor',\n",
    "                             'is_not_sell','is_sell','is_share','is_not_share','is_purpose',\n",
    "                             'is_disclosure','is_security','is_cookies','gunning_fog',\n",
    "                             'smog_index','avg_sentence_length','flesch_reading_ease',\n",
    "                             'dale_chall_readability_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jayap\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# capture text from the document\n",
    "nlp = en_core_web_sm.load()\n",
    "sell_data = [nlp(text) for text in ('whether they sell personal data')]\n",
    "not_sell_data = [nlp(text) for text in ('whether they do not sell personal data')]\n",
    "share_data = [nlp(text) for text in ('they share personal data','protect personal Information')]\n",
    "not_share_data = [nlp(text) for text in ('whether they do not share personal data')]\n",
    "purpose = [nlp(text) for text in ('other purposes', 'how they use it')]\n",
    "disclosure = [nlp(text) for text in ('who is collecting their data', 'they disclose the information collected','other entities are collecting information')]\n",
    "security = [nlp(text) for text in ('security','secure')]\n",
    "#\n",
    "\n",
    "for filename in glob.glob('*.txt'):\n",
    "    with open(filename) as f:\n",
    "        text = (\" \".join(line.strip() for line in f))\n",
    "    file_name = f.name\n",
    "    text = text.lower()\n",
    "\n",
    "    file_id = int(''.join([n for n in file_name if n.isdigit()]))\n",
    "    dale_chall = dale_chall_readability_score(text)\n",
    "    flesch = flesch_reading_ease(text)\n",
    "    smog = smog_index(text)\n",
    "    gunning = gunning_fog(text)\n",
    "    avg_sentence_len = avg_sentence_length(text)\n",
    "\n",
    "    minor_search = int(bool(re.search('minor', text))) \n",
    "    geo_search = int(bool(re.search('geo-location', text)))\n",
    "    email_search = int(bool(re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)))\n",
    "    vendor_search = int(bool(re.search('vendor', text)))\n",
    "    \n",
    "    parser_output = pharse_test(nlp,sell_data,not_sell_data,share_data,\n",
    "                                 not_share_data,purpose,disclosure,security)\n",
    "    \n",
    "    cookies_search = Stemming_word(text)\n",
    "    if cookies_search == None:\n",
    "        cookies_search = 0\n",
    "\n",
    "    dict_value = {'id':[file_id], \n",
    "                  'is_minor':[minor_search],\n",
    "                  'is_geo-location':[geo_search],\n",
    "                  'is_email':[email_search],\n",
    "                  'is_vendor':[vendor_search],\n",
    "                  'is_sell':[parser_output[0]],\n",
    "                  'is_not_sell':[parser_output[1]],\n",
    "                  'is_share':[parser_output[2]],\n",
    "                  'is_not_share':[parser_output[3]],\n",
    "                  'is_purpose':[parser_output[4]],\n",
    "                  'is_disclosure':[parser_output[5]],\n",
    "                  'is_security':[parser_output[6]],\n",
    "                  'is_cookies':[cookies_search],\n",
    "                  'gunning_fog':[gunning],\n",
    "                  'smog_index':[smog],\n",
    "                  'avg_sentence_length':[avg_sentence_len],\n",
    "                  'flesch_reading_ease':[flesch],\n",
    "                  'dale_chall_readability_score':[dale_chall]}\n",
    "\n",
    "    \n",
    "    df1 = pd.DataFrame.from_dict(dict_value)\n",
    "    df = pd.concat([df,df1])\n",
    "df = df.reset_index()\n",
    "df = df[['id','is_minor','is_geo-location','is_email','is_vendor',\n",
    "         'is_not_sell','is_sell','is_share','is_not_share',\n",
    "         'is_cookies','gunning_fog','smog_index','avg_sentence_length',\n",
    "         'flesch_reading_ease','dale_chall_readability_score']]\n",
    "\n",
    "file_name = 'GCSI_Thread_1.csv'\n",
    "df.to_csv(file_name, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
